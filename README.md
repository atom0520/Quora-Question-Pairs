# Quora Question Pairs
### Data Mining Course Project
----------------------------------------
<b>Team Members: Jerry Chen, Atom (Shufan) Cai, Yuying Li</b>

<br/>
This project deals with the problem of identifying duplicate Quora questions, which is originated from a Kaggle competition (https://www.kaggle.com/c/quora-question-pairs). We use an ensemble model averaging an LSTM based deep network and a XGboost model as our solution, and have got a great score of 0.12944 in private leaderboard.

<br/>
<br/>

## Prerequisites

 * This project is written in Python 3.5 using Jupyter Notebook (http://jupyter.org/). Testing it in Python 2.x will have the consequence of integer truncation where most features values will end up with being 0â€™s.
 
 * Required Packages
 
	1. Regular expressions (import **re**)
	2. Natural Language Toolkit (import **nltk**) 
	3. Pandas data analysis library (import **pandas**)
	4. Fundamental NumPy algebra package (import **numpy**)
	5. String matching package fuzzywuzzy (import **fuzzywuzzy**)
	6. Utilities for comparing sequences (import **distance**)
	7. High-performance container datatypes (import **collections**)
	8. Data structures for graphs (import **networkx**)
	9. Machine learning in Python scikit-learn (import **sklearn**)
	10. High-level neural networks API Keras (import **keras**)
 
 * Download the pre-trained word vectors at https://nlp.stanford.edu/projects/glove/ (5.65GB) and this should be placed in the project directory.
 
 * Training data and test data can be downloaded at the Kaggle link below and they should be placed in the "data" folder. 
   https://www.kaggle.com/c/quora-question-pairs/data

 * Prediction files would be generated by the program in the "predictions" folder.

<br/>

## Test our code

There are 5 Python scripts in this repository, namely: 15_nlp_features.ipynb, 6_non_nlp_Features.ipynb, xgboost.ipynb, lstm_dn.ipynb, and ensembling_postprocessing.ipynb.

The brief description for each script is as below:

 * <b>15_nlp_features.ipynb</b><br/>
This script will generate two .csv files "nlp_stemmed_features_train.csv" and "nlp_stemmed_features_test.csv" with 15 columns each (10 token features + 5 fuzzy features after pre-processing). See the script for more details.

 * <b>6_non_nlp_features.ipynb</b><br/>
This script will generate two .csv files "non_nlp_features_train.csv" and "non_nlp_features_test.csv" with 6 columns each (magic features after pre-processing). See the script for more details.

 * <b>xgboost.ipynb</b><br/>
This script is for our XGBoost model, which will generate one prediction file named "xgb_preds.csv" under the "predictions" folder.

 * <b>lstm_dn.ipynb</b><br/>
This script is for our LSTM based deep network model, which will generate 4 additional features and trains a LSTM based deep network model to make 10 predictions using the 10-folds cross validation technique. The 10 prediction files are named from "dn_pred0.csv" to "dn_pred9.csv" and will be saved under the "predictions" folder.

 * <b>ensembling_postprocessing.ipynb</b><br/>
This script will average the prediction from XGBoost model and 10 predictions from the deep network model. It will also further process the average predition and output a final prediction file named "averaged_xgb_dn_preds_post.csv" as our final submission.

<b>Note:</b> Run the above scripts from top to bottom. The whole process may take ~12 hours depending on the processor and GPU support.
