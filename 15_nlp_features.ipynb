{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda2\\envs\\tensorflow\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:35: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import re # Regular expression package.\n",
    "import pandas as pd # Data analysis library used in processing data and features.\n",
    "from nltk.corpus import stopwords # Used in pre-processing to remove stopwords.\n",
    "from fuzzywuzzy import fuzz # String matching package used in extracting fuzzy features.\n",
    "import distance # Used to compare sequences of tokens.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer # Used in pre-processing to stem words.\n",
    "from collections import Counter # Used in processing strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevent divison-by-zero errors by adding SAFE_DIV to the demominator.\n",
    "SAFE_DIV = 0.000001 \n",
    "# Words that appear < RARE_WORD_LIMIT are considered as rare words.\n",
    "RARE_WORD_LIMIT = 100 \n",
    "# Rare words are replaced by this magic_word which is referenced from \n",
    "# the article \"Wonderful Words That You're Not Using (Yet)\"\n",
    "magic_word = 'biblioklept' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing words with length greater than 4.\n",
    "def cutter(word):\n",
    "    if len(word) < 4:\n",
    "        return word\n",
    "    return WordNetLemmatizer().lemmatize(WordNetLemmatizer().lemmatize(word, \"n\"), \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning texts\n",
    "def preprocess(string):\n",
    "    \n",
    "    string = string.lower() # Lower capitalizations.\n",
    "    \n",
    "    string.replace(\",000,000\", \"m\")\n",
    "    string.replace(\",000\", \"k\")\n",
    "    string.replace(\"′\", \"'\")\n",
    "    string.replace(\"’\", \"'\")\n",
    "    string.replace(\"won't\", \"will not\")\n",
    "    string.replace(\"cannot\", \"can not\")\n",
    "    string.replace(\"can't\", \"can not\")\n",
    "    string.replace(\"n't\", \" not\")\n",
    "    string.replace(\"what's\", \"what is\")\n",
    "    string.replace(\"that's\", \"that is\")\n",
    "    string.replace(\"it's\", \"it is\")\n",
    "    string.replace(\"'ve\", \" have\")\n",
    "    string.replace(\"i'm\", \"i am\")\n",
    "    string.replace(\"'re\", \" are\")\n",
    "    string.replace(\"he's\", \"he is\")\n",
    "    string.replace(\"she's\", \"she is\")\n",
    "    string.replace(\"'s\", \" own\")\n",
    "    string.replace(\"%\", \" percent \")\n",
    "    string.replace(\"₹\", \" rupee \")\n",
    "    string.replace(\"$\", \" dollar \")\n",
    "    string.replace(\"€\", \" euro \")\n",
    "    string.replace(\"'ll\", \" will\")\n",
    "    string.replace(\"'d\", \" would\")\n",
    "    string.replace(\"=\", \" equal \")\n",
    "    string.replace(\"+\", \" plus \")\n",
    "        \n",
    "    string = re.sub(r\"e-mail\", \"email\", string)\n",
    "    string = re.sub(r\" usa \", \" america \", string)\n",
    "    string = re.sub(r\"the us\", \"america\", string)\n",
    "    string = re.sub(r\" uk \", \" england \", string)\n",
    "    string = re.sub(r\"c#\", \"c sharp\", string)\n",
    "    \n",
    "    string = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', string)\n",
    "    string = re.sub(r\"([0-9]+)000000\", r\"\\1m\", string)\n",
    "    string = re.sub(r\"([0-9]+)000\", r\"\\1k\", string)\n",
    "    \n",
    "    string = ' '.join([cutter(w) for w in string.split()])\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_features(q1, q2):\n",
    "    \n",
    "    STOP_WORDS = stopwords.words(\"english\") \n",
    "    token_features = [0.0]*10\n",
    "\n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "\n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "\n",
    "    # Word list in Question1 excluding stop words.\n",
    "    q1_words = set([cutter(word) for word in q1_tokens if word not in STOP_WORDS])\n",
    "    # Word list in Question2 excluding stop words.\n",
    "    q2_words = set([cutter(word) for word in q2_tokens if word not in STOP_WORDS])\n",
    "    # Stop word list in Question1.\n",
    "    q1_stops = set([cutter(word) for word in q1_tokens if word in STOP_WORDS])\n",
    "    # Stop word list in Question2.\n",
    "    q2_stops = set([cutter(word) for word in q2_tokens if word in STOP_WORDS])\n",
    "\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "\n",
    "    # These NLP features are extracted using min and max operations instead of being question dependent.\n",
    "    # All these features thus have a better generalization.\n",
    "    \n",
    "    # Common word ratio with respect to the shorter question excluding stop words.\n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    # Common word ratio with respect to the longer question excluding stop words.\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    # Common word ratio with respect to the shorter stop word list.\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    # Common word ratio with respect to the longer stop word list.\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    # Common token ratio with respect to the shorter question.\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    # Common token ratio with respect to the longer question.\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    # Equality of the last word.\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1]) \n",
    "    # Equality of the first word.\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0]) \n",
    "    # Difference in lengths of two questions.\n",
    "    token_features[8] = abs(len(q1_tokens) - len(q2_tokens)) \n",
    "    # Average length of two questions.\n",
    "    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2 \n",
    "    return token_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_substr_ratio(a, b):\n",
    "    strs = list(distance.lcsubstrings(a, b))\n",
    "    if len(strs) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(strs[0]) / (min(len(a), len(b)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words that appear less than 100 times are considered as \"rare words\" and replaced by a special token.\n",
    "def replace_rare(df):\n",
    "\n",
    "    words = []\n",
    "    for q in df['question1']:\n",
    "        for w in q.split():\n",
    "            words.append(w);\n",
    "    for q in df['question2']:\n",
    "        for w in q.split():\n",
    "            words.append(w);\n",
    "\n",
    "    counts = Counter(words)\n",
    "    rare_words = []\n",
    "    for w,c in counts.items():\n",
    "        if c < RARE_WORD_LIMIT:\n",
    "            rare_words.append(w)\n",
    "\n",
    "    result_df_q1 = []\n",
    "    for q in df['question1']:\n",
    "        result_df_q1.append([word if word not in rare_words else magic_word for word in q])\n",
    "\n",
    "    result_df_q2 = []\n",
    "    for q in df['question2']:\n",
    "        result_df_q1.append([word if word not in rare_words else magic_word for word in q])\n",
    "\n",
    "    df['question1'] = result_df_q1\n",
    "    df['question2'] = result_df_q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n",
    "    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n",
    "\n",
    "    replace_rare(df) # Replace rare words with a special token.\n",
    "\n",
    "    print(\"Token features.\")\n",
    "    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
    "    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
    "    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
    "    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
    "    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
    "    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
    "    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
    "    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n",
    "    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n",
    "    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n",
    "\n",
    "    print(\"Fuzzy features.\")\n",
    "    # Ratio of removing duplicates.\n",
    "    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    # Ratio of ordered words.\n",
    "    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    # Ratio of original questions.\n",
    "    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    # Ratio of similar substring.\n",
    "    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    # Ratio of longer substring over the shorter one.\n",
    "    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for training data:\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features for training data:\")\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "train_df = extract_features(train_df)\n",
    "train_df.drop([\"id\", \"qid1\", \"qid2\", \"question1\", \"question2\", \"is_duplicate\"], axis=1, inplace=True)\n",
    "train_df.to_csv(\"data/nlp_stemmed_features_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting features for testing data:\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "test_df = extract_features(test_df)\n",
    "test_df.drop([\"test_id\", \"question1\", \"question2\"], axis=1, inplace=True)\n",
    "test_df.to_csv(\"data/nlp_stemmed_features_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
