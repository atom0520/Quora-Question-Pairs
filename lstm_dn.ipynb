{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Bidirectional\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import concatenate, add, multiply\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "WNL = WordNetLemmatizer() # Used to lemmatize words\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "MAX_SEQUENCE_LENGTH = 30 # The word sequence length of each processed question sentence\n",
    "MIN_WORD_OCCURRENCE = 100 # The words/phrases that occurr lower than this number of times will be replaced with the following REPLACE_WORD\n",
    "REPLACE_WORD = \"memento\" # The word use to replace the words/phrases that occurr lower than specific times\n",
    "EMBEDDING_DIM = 300 # The dimension of each word embedding vector\n",
    "NUM_FOLDS = 10 # The number of folds in the k-folds cross-valid trainig\n",
    "BATCH_SIZE = 1025 # The used batch size in the training\n",
    "EMBEDDING_FILE = \"glove.840B.300d.txt\" # The filename of the GloVe Word Embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing words with length greater than 4.\n",
    "def cutter(word):\n",
    "    if len(word) < 4:\n",
    "        return word\n",
    "    return WNL.lemmatize(WNL.lemmatize(word, \"n\"), \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning texts\n",
    "def preprocess(string):\n",
    "    \n",
    "    string = string.lower()\n",
    "    \n",
    "    string.replace(\",000,000\", \"m\")\n",
    "    string.replace(\",000\", \"k\")\n",
    "    string.replace(\"′\", \"'\")\n",
    "    string.replace(\"’\", \"'\")\n",
    "    string.replace(\"won't\", \"will not\")\n",
    "    string.replace(\"cannot\", \"can not\")\n",
    "    string.replace(\"can't\", \"can not\")\n",
    "    string.replace(\"n't\", \" not\")\n",
    "    string.replace(\"what's\", \"what is\")\n",
    "    string.replace(\"that's\", \"that is\")\n",
    "    string.replace(\"it's\", \"it is\")\n",
    "    string.replace(\"'ve\", \" have\")\n",
    "    string.replace(\"i'm\", \"i am\")\n",
    "    string.replace(\"'re\", \" are\")\n",
    "    string.replace(\"he's\", \"he is\")\n",
    "    string.replace(\"she's\", \"she is\")\n",
    "    string.replace(\"'s\", \" own\")\n",
    "    string.replace(\"%\", \" percent \")\n",
    "    string.replace(\"₹\", \" rupee \")\n",
    "    string.replace(\"$\", \" dollar \")\n",
    "    string.replace(\"€\", \" euro \")\n",
    "    string.replace(\"'ll\", \" will\")\n",
    "    string.replace(\"'d\", \" would\")\n",
    "    string.replace(\"=\", \" equal \")\n",
    "    string.replace(\"+\", \" plus \")\n",
    "        \n",
    "    string = re.sub(r\"e-mail\", \"email\", string)\n",
    "  \n",
    "    string = re.sub(r\" usa \", \" america \", string)\n",
    "\n",
    "    string = re.sub(r\"the us\", \"america\", string)\n",
    "    string = re.sub(r\" uk \", \" england \", string)\n",
    "    string = re.sub(r\"c#\", \"c sharp\", string)\n",
    "    \n",
    "    string = re.sub(r\" cs \", \" computer science \", string) \n",
    "    \n",
    "    string = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', string)\n",
    "    string = re.sub(r\"([0-9]+)000000\", r\"\\1m\", string)\n",
    "    string = re.sub(r\"([0-9]+)000\", r\"\\1k\", string)\n",
    "    \n",
    "    string = ' '.join([cutter(w) for w in string.split()])\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and return a map of the high frequency words' embeddings from the file of GloVe Word Embeddings\n",
    "def get_embedding():\n",
    "    embeddings_index = {}\n",
    "    f = open(EMBEDDING_FILE, encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if len(values) == EMBEDDING_DIM + 1 and word in top_words:\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return if one word is numeric by checking if it contains any digit character\n",
    "def is_numeric(s):\n",
    "    return any(i.isdigit() for i in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a question sentence as an input and output one list and two sets\n",
    "# The ouput new_q list is the input sentence's word list with all the low frequency words/phrases replaced with \"memento\"\n",
    "# The ouput surplus_q set is the non-numeric word set of the input sentence\n",
    "# The ouput numbers_q set is the numeric word set of the input sentence\n",
    "def prepare(q):\n",
    "    new_q = []\n",
    "    surplus_q = []\n",
    "    numbers_q = []\n",
    "    new_memento = True\n",
    "    for w in q.split()[::-1]:\n",
    "        if w in top_words:\n",
    "            new_q = [w] + new_q\n",
    "            new_memento = True\n",
    "        elif w not in STOP_WORDS:\n",
    "            if new_memento:\n",
    "                new_q = [\"memento\"] + new_q\n",
    "                new_memento = False\n",
    "            if is_numeric(w):\n",
    "                numbers_q = [w] + numbers_q\n",
    "            else:\n",
    "                surplus_q = [w] + surplus_q\n",
    "        else:\n",
    "            new_memento = True\n",
    "        if len(new_q) == MAX_SEQUENCE_LENGTH:\n",
    "            break\n",
    "    new_q = \" \".join(new_q)\n",
    "    return new_q, set(surplus_q), set(numbers_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the following four additional features from the data set:\n",
    "# The length of the intersection set of two questions' non-numeric word sets\n",
    "# The length of the union set of two questions' non-numeric word sets\n",
    "# The length of the intersection set of two questions' numeric word sets\n",
    "# The length of the union set of two questions' numeric word sets\n",
    "def extract_features(df):\n",
    "    q1s = np.array([\"\"] * len(df), dtype=object)\n",
    "    q2s = np.array([\"\"] * len(df), dtype=object)\n",
    "    features = np.zeros((len(df), 4))\n",
    "\n",
    "    for i, (q1, q2) in enumerate(list(zip(df[\"question1\"], df[\"question2\"]))):\n",
    "        q1s[i], surplus1, numbers1 = prepare(q1)\n",
    "        q2s[i], surplus2, numbers2 = prepare(q2)\n",
    "        features[i, 0] = len(surplus1.intersection(surplus2))\n",
    "        features[i, 1] = len(surplus1.union(surplus2))\n",
    "        features[i, 2] = len(numbers1.intersection(numbers2))\n",
    "        features[i, 3] = len(numbers1.union(numbers2))\n",
    "\n",
    "    return q1s, q2s, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# Clean texts for all the question sentences in the training data\n",
    "train[\"question1\"] = train[\"question1\"].fillna(\"\").apply(preprocess)\n",
    "train[\"question2\"] = train[\"question2\"].fillna(\"\").apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vocabulary of words occurred more than 100\n",
      "Words are not found in the embedding: {'brexit', 'americaes', '$100', 'don’t', 'what’s', '100%', 'americaa', 'redmi', 'oneplus', 'americae', 'iisc', 'demonetisation', 'kvpy', 'quorans', 'i’m', '\\\\frac', '$1', 'paytm'}\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabulary of words that occurr more than specific times\n",
    "print(\"Creating the vocabulary of words occurred more than\", MIN_WORD_OCCURRENCE)\n",
    "all_questions = pd.Series(train[\"question1\"].tolist() + train[\"question2\"].tolist()).unique()\n",
    "vectorizer = CountVectorizer(lowercase=False, token_pattern=\"\\S+\", min_df=MIN_WORD_OCCURRENCE)\n",
    "vectorizer.fit(all_questions)\n",
    "top_words = set(vectorizer.vocabulary_.keys())\n",
    "top_words.add(REPLACE_WORD)\n",
    "\n",
    "# Generate the map of high frequency words' embeddings \n",
    "embeddings_index = get_embedding()\n",
    "\n",
    "print(\"Words are not found in the embedding:\", top_words - embeddings_index.keys())\n",
    "top_words = embeddings_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train questions are being prepared for LSTM...\n"
     ]
    }
   ],
   "source": [
    "print(\"Train questions are being prepared for LSTM...\")\n",
    "# Process the training data and extract some additional features named \"q features\" for the training data\n",
    "q1s_train, q2s_train, train_q_features = extract_features(train)\n",
    "\n",
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(np.append(q1s_train, q2s_train))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Turn training data questions into word sequences. Then Pad or truncate each word sequence to be the same length\n",
    "data_1 = pad_sequences(tokenizer.texts_to_sequences(q1s_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(tokenizer.texts_to_sequences(q2s_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(train[\"is_duplicate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the word embedding matrix from the map of high frequency words' embeddings \n",
    "nb_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features are being merged with NLP and Non-NLP features...\n",
      "Same steps are being applied for test...\n"
     ]
    }
   ],
   "source": [
    "# Merge NLP features, Non-NLP features, and q features of the training data to get the final training data features\n",
    "print(\"Train features are being merged with NLP and Non-NLP features...\")\n",
    "train_nlp_features = pd.read_csv(\"data/nlp_stemmed_features_train.csv\")\n",
    "train_non_nlp_features = pd.read_csv(\"data/non_nlp_features_train.csv\")\n",
    "features_train = np.hstack((train_q_features, train_nlp_features, train_non_nlp_features))\n",
    "\n",
    "print(\"Same steps are being applied for test...\")\n",
    "# Clean texts for all the question sentences in the test data\n",
    "test[\"question1\"] = test[\"question1\"].fillna(\"\").apply(preprocess)\n",
    "test[\"question2\"] = test[\"question2\"].fillna(\"\").apply(preprocess)\n",
    "\n",
    "# Process the test data and extract some additional features named \"q features\" for the test data\n",
    "q1s_test, q2s_test, test_q_features = extract_features(test)\n",
    "\n",
    "# Turn test data questions into word sequences. Then Pad or truncate each word sequence to be the same length\n",
    "test_data_1 = pad_sequences(tokenizer.texts_to_sequences(q1s_test), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(tokenizer.texts_to_sequences(q2s_test), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Merge NLP features, Non-NLP features, and q features of the test data to get the final test data features\n",
    "test_nlp_features = pd.read_csv(\"data/nlp_stemmed_features_test.csv\")\n",
    "test_non_nlp_features = pd.read_csv(\"data/non_nlp_features_test.csv\")\n",
    "features_test = np.hstack((test_q_features, test_nlp_features, test_non_nlp_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: 0\n",
      "Train on 363860 samples, validate on 40430 samples\n",
      "Epoch 1/15\n",
      "318775/363860 [=========================>....] - ETA: 1:19 - loss: 0.2781"
     ]
    }
   ],
   "source": [
    "# A class instance used to divide the training data to do cross validation training\n",
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True)\n",
    "model_count = 0\n",
    "\n",
    "# K-folds cross validation training loop\n",
    "for idx_train, idx_val in skf.split(train[\"is_duplicate\"], train[\"is_duplicate\"]):\n",
    "    print(\"MODEL:\", model_count)\n",
    "    # The question 1 word sequences of the training set\n",
    "    data_1_train = data_1[idx_train]\n",
    "    # The question 2 word sequences of the training set\n",
    "    data_2_train = data_2[idx_train]\n",
    "    # The labels of the training set\n",
    "    labels_train = labels[idx_train]\n",
    "    # The features of the training set\n",
    "    f_train = features_train[idx_train]\n",
    "\n",
    "    # The question 1 word sequences of the validation set\n",
    "    data_1_val = data_1[idx_val]\n",
    "    # The question 2 word sequences of the validation set\n",
    "    data_2_val = data_2[idx_val]\n",
    "    # The labels of the validation set\n",
    "    labels_val = labels[idx_val]\n",
    "    # The features of the validation set\n",
    "    f_val = features_train[idx_val]\n",
    "    \n",
    "    # Word embedding layer\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    \n",
    "    # LSTM layer\n",
    "    lstm_layer = LSTM(75, recurrent_dropout=0.2)\n",
    "\n",
    "    # The input question 1 word sequence\n",
    "    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    # The word embeddings of question 1\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    # The sentense representation of question 1 generated by LSTM\n",
    "    x1 = lstm_layer(embedded_sequences_1)\n",
    "    \n",
    "    # The input question 2 word sequence\n",
    "    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    # The word embeddings of question 2\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    # The sentense representation of question 2 generated by LSTM\n",
    "    y1 = lstm_layer(embedded_sequences_2)\n",
    "    \n",
    "    # The input features\n",
    "    features_input = Input(shape=(f_train.shape[1],), dtype=\"float32\")\n",
    "    features_dense = BatchNormalization()(features_input)\n",
    "    features_dense = Dense(200, activation=\"relu\")(features_dense)\n",
    "    features_dense = Dropout(0.2)(features_dense)\n",
    "    \n",
    "    # Add up the representations of question 1 and question 2\n",
    "    addition = add([x1, y1])\n",
    "    \n",
    "    # Get the squared difference of the representations of question 1 and question 2\n",
    "    minus_y1 = Lambda(lambda x: -x)(y1)\n",
    "    merged = add([x1, minus_y1])\n",
    "    merged = multiply([merged, merged])\n",
    "    \n",
    "    # Merge two sentence representations' addition and squared difference to get their interaction tensor\n",
    "    merged = concatenate([merged, addition])\n",
    "   \n",
    "    merged = Dropout(0.4)(merged)\n",
    "\n",
    "    # Merge two sentences' representations interaction and their other features\n",
    "    merged = concatenate([merged, features_dense])\n",
    "\n",
    "    # Then let the merged tensor pass through the following layers \n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = GaussianNoise(0.1)(merged)\n",
    "    merged = Dense(150, activation=\"relu\")(merged)\n",
    "    merged = Dropout(0.2)(merged)\n",
    "    \n",
    "    merged = BatchNormalization()(merged)\n",
    "    \n",
    "    # The output layer outputs the final prediction value\n",
    "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    # Build the model\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input, features_input], outputs=out)\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=\"nadam\")\n",
    "    \n",
    "    # The early stopping callback func which can stop the training if validation loss hasn't improved in specific rounds\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    \n",
    "    # The filepath where to save the best model weights that can produce the lowest validation loss in the current fold's training\n",
    "    best_model_path = \"best_model\" + str(model_count) + \".h5\"\n",
    "    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    # Train the model using the prepared data\n",
    "    hist = model.fit([data_1_train, data_2_train, f_train], labels_train,\n",
    "                     validation_data=([data_1_val, data_2_val, f_val], labels_val),\n",
    "                     epochs=15, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                     callbacks=[early_stopping, model_checkpoint], verbose=1)\n",
    "\n",
    "    # Load the best model weights of the current fold's training\n",
    "    model.load_weights(best_model_path)\n",
    "    \n",
    "    print(model_count, \"validation loss:\", min(hist.history[\"val_loss\"]))\n",
    "\n",
    "    # Use the trained model to predict the test data's prediction values\n",
    "    preds = model.predict([test_data_1, test_data_2, features_test], batch_size=BATCH_SIZE, verbose=1)\n",
    "    \n",
    "    # Save the prediction results of the current fold to file\n",
    "    submission = pd.DataFrame({\"test_id\": test[\"test_id\"], \"is_duplicate\": preds.ravel()})\n",
    "    submission.to_csv(\"predictions/dn_preds\" + str(model_count) + \".csv\", index=False)\n",
    "    \n",
    "    # Go to the next fold\n",
    "    model_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
