{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda2\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features_train_nlp = pd.read_csv('data/nlp_stemmed_features_train.csv')\n",
    "features_train_non_nlp = pd.read_csv('data/non_nlp_features_train.csv')\n",
    "features_test_nlp = pd.read_csv('data/nlp_stemmed_features_test.csv')\n",
    "features_test_non_nlp = pd.read_csv('data/non_nlp_features_test.csv')\n",
    "\n",
    "features_train = pd.concat([features_train_nlp, features_train_non_nlp], axis=1)\n",
    "features_test = pd.concat([features_test_nlp, features_test_non_nlp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebalancing the Data.\n",
    "\n",
    "x_train = pd.DataFrame()\n",
    "\n",
    "x_train.head()\n",
    "\n",
    "x_test = pd.DataFrame()\n",
    "\n",
    "old_y_train = df_train['is_duplicate'].values\n",
    "\n",
    "pos_train = features_train[old_y_train == 1]\n",
    "neg_train = features_train[old_y_train == 0]\n",
    "pos_train.head()\n",
    "neg_train.head()\n",
    "\n",
    "# Balance the positive cases and negative cases\n",
    "p = 0.165\n",
    "scale = (float(len(pos_train) / float(len(pos_train) + len(neg_train)) ) / p) - 1\n",
    "while scale > 1:\n",
    "    neg_train = pd.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "\n",
    "x_train = pd.concat([pos_train, neg_train])\n",
    "y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 10\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.678215\tvalid-logloss:0.67833\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[10]\ttrain-logloss:0.556353\tvalid-logloss:0.557549\n",
      "[20]\ttrain-logloss:0.469778\tvalid-logloss:0.471935\n",
      "[30]\ttrain-logloss:0.406068\tvalid-logloss:0.409102\n",
      "[40]\ttrain-logloss:0.358047\tvalid-logloss:0.361921\n"
     ]
    }
   ],
   "source": [
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "bst = xgb.train(params, d_train, 2500, watchlist, early_stopping_rounds=50, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = xgb.DMatrix(features_test)\n",
    "p_test = bst.predict(d_test)\n",
    "\n",
    "p_test_df = pd.DataFrame({\"test_id\":df_test[\"test_id\"], \"is_duplicate\":p_test.ravel()})\n",
    "p_test_df = p_test_df.reindex(columns=['test_id','is_duplicate'])\n",
    "p_test_df.to_csv('predictions/xgb_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
