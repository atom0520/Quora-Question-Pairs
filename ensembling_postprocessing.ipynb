{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MODELS = 10\n",
    "TRAIN_TARGET_MEAN = 0.37 # The positive class ratio of the training data\n",
    "TEST_TARGET_MEAN = 0.16 # The positive class ratio of the test data\n",
    "REPEAT = 2 # The repeat times of the last two post processing steps\n",
    "DUP_THRESHOLD = 0.3 # Used to determine if a pair of questions are duplicate\n",
    "NOT_DUP_THRESHOLD = 0.1 # Used to determine if a pair of questions are non-duplicate\n",
    "MAX_UPDATE = 0.2 # The maximum update value for each time's prediction update in the last two post processing steps\n",
    "DUP_UPPER_BOUND = 0.98 # Used to determine whether and how much to increase the prediction of two questions with common duplicates\n",
    "NOT_DUP_LOWER_BOUND = 0.01 # Used to determine whether and how much to decrease the prediction of two questions with common non-duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average 1 prediction of XGBoost and 10 predictions from the 10-folds cross-valid trainig using deep network model\n",
    "df = pd.read_csv(\"predictions/dn_preds0.csv\")\n",
    "for i in range(1, NUM_MODELS):\n",
    "    df[\"is_duplicate\"] = df[\"is_duplicate\"] + pd.read_csv(\"predictions/dn_preds\" + str(i) + \".csv\")[\"is_duplicate\"]\n",
    "df[\"is_duplicate\"] = df[\"is_duplicate\"] + pd.read_csv(\"predictions/xgb_preds\" + \".csv\")[\"is_duplicate\"]\n",
    "df[\"is_duplicate\"] /= 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting predictions considering the different class inbalance ratio...\n"
     ]
    }
   ],
   "source": [
    "# Adjust the predictions according to the positive class ratios of training data and test data\n",
    "print(\"Adjusting predictions considering the different class inbalance ratio...\")\n",
    "a = TEST_TARGET_MEAN / TRAIN_TARGET_MEAN\n",
    "b = (1 - TEST_TARGET_MEAN) / (1 - TRAIN_TARGET_MEAN)\n",
    "df[\"is_duplicate\"] = df[\"is_duplicate\"].apply(lambda x: a*x / (a*x + b*(1 - x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = np.array(df[\"is_duplicate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the predictions of the pairs with common duplicates..\n",
      "Updated: 16463\n",
      "Updated: 17298\n"
     ]
    }
   ],
   "source": [
    "# Update the predictions of quetion pairs that have common duplicates\n",
    "# For the underneath principle please see the Post-Processing part in our final report.\n",
    "print(\"Updating the predictions of the pairs with common duplicates..\")\n",
    "for i in range(REPEAT):\n",
    "    dup_neighbors = defaultdict(set)\n",
    "\n",
    "    # Gather each question's duplicate questions based on the training set labels\n",
    "    for dup, q1, q2 in zip(df_train[\"is_duplicate\"], df_train[\"question1\"], df_train[\"question2\"]):\n",
    "        if dup:\n",
    "            dup_neighbors[q1].add(q2)\n",
    "            dup_neighbors[q2].add(q1)\n",
    "\n",
    "    # Gather each question's duplicate questions based on the test set predictions\n",
    "    for dup, q1, q2 in zip(test_label, df_test[\"question1\"], df_test[\"question2\"]):\n",
    "        if dup > DUP_THRESHOLD:\n",
    "            dup_neighbors[q1].add(q2)\n",
    "            dup_neighbors[q2].add(q1)\n",
    "\n",
    "    # If a pair of questions have common duplicates then their prediction should not be lower than DUP_UPPER_BOUND, otherwise increase their predition\n",
    "    count = 0\n",
    "    for index, (q1, q2) in enumerate(zip(df_test[\"question1\"], df_test[\"question2\"])):\n",
    "        dup_neighbor_count = len(dup_neighbors[q1].intersection(dup_neighbors[q2]))\n",
    "        if dup_neighbor_count > 0 and test_label[index] < DUP_UPPER_BOUND:\n",
    "            update = min(MAX_UPDATE, (DUP_UPPER_BOUND - test_label[index]) / 2)\n",
    "            test_label[index] += update\n",
    "            count += 1\n",
    "\n",
    "    print(\"Updated:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the predictions of the pairs with common non-duplicates..\n",
      "Updated: 4807\n",
      "Updated: 5142\n"
     ]
    }
   ],
   "source": [
    "# Update the predictions of quetion pairs that have common non-duplicates\n",
    "# For the underneath principle please see the Post-Processing part in our final report.\n",
    "print(\"Updating the predictions of the pairs with common non-duplicates..\")\n",
    "for i in range(REPEAT):\n",
    "    not_dup_neighbors = defaultdict(set)\n",
    "\n",
    "    # Gather each question's non-duplicate questions based on the training set labels\n",
    "    for dup, q1, q2 in zip(df_train[\"is_duplicate\"], df_train[\"question1\"], df_train[\"question2\"]):\n",
    "        if not dup:\n",
    "            not_dup_neighbors[q1].add(q2)\n",
    "            not_dup_neighbors[q2].add(q1)\n",
    "\n",
    "    # Gather each question's non-duplicate questions based on the test set predictions\n",
    "    for dup, q1, q2 in zip(test_label, df_test[\"question1\"], df_test[\"question2\"]):\n",
    "        if dup < NOT_DUP_THRESHOLD:\n",
    "            not_dup_neighbors[q1].add(q2)\n",
    "            not_dup_neighbors[q2].add(q1)\n",
    "\n",
    "    # If a pair of questions have common non-duplicates then their prediction should not be higher than NOT_DUP_LOWER_BOUND, otherwise decrease their predition\n",
    "    count = 0\n",
    "    for index, (q1, q2) in enumerate(zip(df_test[\"question1\"], df_test[\"question2\"])):\n",
    "        dup_neighbor_count = len(not_dup_neighbors[q1].intersection(not_dup_neighbors[q2]))\n",
    "        if dup_neighbor_count > 0 and test_label[index] > NOT_DUP_LOWER_BOUND:\n",
    "            update = min(MAX_UPDATE, (test_label[index] - NOT_DUP_LOWER_BOUND) / 2)\n",
    "            test_label[index] -= update\n",
    "            count += 1\n",
    "\n",
    "    print(\"Updated:\", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final predictions to file\n",
    "submission = pd.DataFrame({\"test_id\":df_test[\"test_id\"], \"is_duplicate\":test_label})\n",
    "submission = submission.reindex(columns=['test_id','is_duplicate'])\n",
    "submission.to_csv(\"predictions/averaged_xgb_dn_preds_post.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
