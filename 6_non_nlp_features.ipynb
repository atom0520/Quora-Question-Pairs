{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CORES = 10\n",
    "FREQ_UPPER_BOUND = 100\n",
    "NEIGHBOR_UPPER_BOUND = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hash table with all questions form both train and test data,\n",
    "# now each question has a corresponding integer as hash value.\n",
    "def create_question_hash(train_df, test_df):\n",
    "    train_qs = np.dstack([train_df[\"question1\"], train_df[\"question2\"]]).flatten()\n",
    "    test_qs = np.dstack([test_df[\"question1\"], test_df[\"question2\"]]).flatten()\n",
    "    all_qs = np.append(train_qs, test_qs)\n",
    "    all_qs = pd.DataFrame(all_qs)[0].drop_duplicates()\n",
    "    all_qs.reset_index(inplace=True, drop=True)\n",
    "    question_dict = pd.Series(all_qs.index.values, index=all_qs.values).to_dict()\n",
    "    return question_dict\n",
    "\n",
    "# Now the returned dataframe contains hashing values representing the original questions.\n",
    "def get_hash(df, hash_dict):\n",
    "    df[\"qid1\"] = df[\"question1\"].map(hash_dict)\n",
    "    df[\"qid2\"] = df[\"question2\"].map(hash_dict)\n",
    "    return df.drop([\"question1\", \"question2\"], axis=1)\n",
    "\n",
    "# Returns a dict such that all hash values are paired with number of unique neighbours up to NB_CORES + 1.\n",
    "def get_kcore_dict(df):\n",
    "    #generate a graph, then add all nodes(questions hash values) and edges(question pairs)\n",
    "    g = nx.Graph()\n",
    "    g.add_nodes_from(df.qid1)\n",
    "    edges = list(df[[\"qid1\", \"qid2\"]].to_records(index=False))\n",
    "    g.add_edges_from(edges)\n",
    "    g.remove_edges_from(g.selfloop_edges())\n",
    "\n",
    "    #create a new dataframe such that it contains a column labeled qid, and this column contains all qids from the graph\n",
    "    df_output = pd.DataFrame(data=list(g.nodes()), columns=[\"qid\"])\n",
    "    #add new column called kcore and initialize values to 0\n",
    "    df_output[\"kcore\"] = 0\n",
    "    for k in range(2, NB_CORES + 1):\n",
    "        # retreives maximal subgraph that contains nodes of degree k or more\n",
    "        # so we are able to get all qid's which has been in k or more unique question pairs\n",
    "        ck = nx.k_core(g, k=k).nodes()\n",
    "        print(\"kcore\", k)\n",
    "        # set each questoin's kcore value to the maximum of unique pairs they have(number of their unique neighbours)\n",
    "        df_output.ix[df_output.qid.isin(ck), \"kcore\"] = k\n",
    "\n",
    "    # returns a dictionary with hash value of question as key and kcore as value\n",
    "    return df_output.to_dict()[\"kcore\"]\n",
    "\n",
    "# attach the kcore values from get_kcore_dict() to df using new columns 'kcore1' and 'kcore2'\n",
    "def get_kcore_features(df, kcore_dict):\n",
    "    df[\"kcore1\"] = df[\"qid1\"].apply(lambda x: kcore_dict[x])\n",
    "    df[\"kcore2\"] = df[\"qid2\"].apply(lambda x: kcore_dict[x])\n",
    "    return df\n",
    "\n",
    "# since we want to achieve the same feature values regardless of position of q1 and q2, we keep the kcore/freq values in\n",
    "#min and max order instead of the q1 q2 order\n",
    "def convert_to_minmax(df, col):\n",
    "    sorted_features = np.sort(np.vstack([df[col + \"1\"], df[col + \"2\"]]).T)\n",
    "    df[\"min_\" + col] = sorted_features[:, 0]\n",
    "    df[\"max_\" + col] = sorted_features[:, 1]\n",
    "    return df.drop([col + \"1\", col + \"2\"], axis=1)\n",
    "\n",
    "# returns a dict such that for each question, we can find all unique questions that has been paired up with it, \n",
    "def get_neighbors(train_df, test_df):\n",
    "    neighbors = defaultdict(set)\n",
    "    for df in [train_df, test_df]:\n",
    "        for q1, q2 in zip(df[\"qid1\"], df[\"qid2\"]):\n",
    "            neighbors[q1].add(q2)\n",
    "            neighbors[q2].add(q1)\n",
    "    return neighbors\n",
    "\n",
    "# adds neighbor features to df\n",
    "def get_neighbor_features(df, neighbors):\n",
    "    #find the number of neighbors question1 and question2 share\n",
    "    common_nc = df.apply(lambda x: len(neighbors[x.qid1].intersection(neighbors[x.qid2])), axis=1)\n",
    "    #find the length of the least neighbors between the two\n",
    "    min_nc = df.apply(lambda x: min(len(neighbors[x.qid1]), len(neighbors[x.qid2])), axis=1)\n",
    "    #computer the ratio\n",
    "    df[\"common_neighbor_ratio\"] = common_nc / min_nc\n",
    "    # limit the number of neighbors up to NEIGHBOR_UPPER_BOUND\n",
    "    df[\"common_neighbor_count\"] = common_nc.apply(lambda x: min(x, NEIGHBOR_UPPER_BOUND))\n",
    "    return df\n",
    "\n",
    "# returns the df with added columns 'freq1' and 'freq2' such that they keep the number of corresponding question's appearance\n",
    "def get_freq_features(df, frequency_map):\n",
    "    df[\"freq1\"] = df[\"qid1\"].map(lambda x: min(frequency_map[x], FREQ_UPPER_BOUND))\n",
    "    df[\"freq2\"] = df[\"qid2\"].map(lambda x: min(frequency_map[x], FREQ_UPPER_BOUND))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashing the questions.\n",
      "Number of unique questions: 4789031\n",
      "Calculating kcore features.\n"
     ]
    }
   ],
   "source": [
    "print(\"Hashing the questions.\")\n",
    "question_dict = create_question_hash(train_df, test_df)\n",
    "train_df = get_hash(train_df, question_dict)\n",
    "test_df = get_hash(test_df, question_dict)\n",
    "print(\"Number of unique questions:\", len(question_dict))\n",
    "\n",
    "print(\"Calculating kcore features.\")\n",
    "all_df = pd.concat([train_df, test_df])\n",
    "kcore_dict = get_kcore_dict(all_df)\n",
    "train_df = get_kcore_features(train_df, kcore_dict)\n",
    "test_df = get_kcore_features(test_df, kcore_dict)\n",
    "train_df = convert_to_minmax(train_df, \"kcore\")\n",
    "test_df = convert_to_minmax(test_df, \"kcore\")\n",
    "\n",
    "print(\"Calculating common neighbor features.\")\n",
    "neighbors = get_neighbors(train_df, test_df)\n",
    "train_df = get_neighbor_features(train_df, neighbors)\n",
    "test_df = get_neighbor_features(test_df, neighbors)\n",
    "\n",
    "print(\"Calculating frequency features...\")\n",
    "# find the number of each question appeared in total\n",
    "frequency_map = dict(zip(*np.unique(np.vstack((all_df[\"qid1\"], all_df[\"qid2\"])), return_counts=True)))\n",
    "train_df = get_freq_features(train_df, frequency_map)\n",
    "test_df = get_freq_features(test_df, frequency_map)\n",
    "train_df = convert_to_minmax(train_df, \"freq\")\n",
    "test_df = convert_to_minmax(test_df, \"freq\")\n",
    "\n",
    "#the features are:\n",
    "#min_kcore: number of unique neighbours that question in pair has less of\n",
    "#max_kcore: number of unique neighbours that question in pair has more of\n",
    "#common_neighbor_count: number of common neighbours the question pair shares\n",
    "#common_neighbor_ratio: the ratio of number of their shared neighbour over the lesser neighbour count question\n",
    "#min_freq: the number of appearance for the question that has less of\n",
    "#max_freq: the number of appearance for the question that has more of\n",
    "cols = [\"min_kcore\", \"max_kcore\", \"common_neighbor_count\", \"common_neighbor_ratio\", \"min_freq\", \"max_freq\"]\n",
    "train_df.loc[:, cols].to_csv(\"data/non_nlp_features_train.csv\", index=False)\n",
    "test_df.loc[:, cols].to_csv(\"data/non_nlp_features_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
